# 书海阁小说爬虫

一个高效、智能的 Python 小说爬虫工具，专为书海阁小说网（m.shuhaige.net）设计，支持并发下载、自动分页处理和内容清理。

## ✨ 主要特性

### 🚀 高性能
- **并发URL收集**：使用线程池和队列机制，并发发现和收集所有章节页面
- **并发内容下载**：多线程同时下载多个页面内容，显著提升爬取速度
- **会话复用**：使用 `requests.Session` 保持连接，减少网络开销

### 🎯 智能处理
- **自动分页识别**：智能识别同章节内的分页（`_1.html`, `_2.html` 等）
- **章节顺序保证**：基于URL解析的章节ID和页码排序，确保内容顺序正确
- **内容自动清理**：过滤广告、水印等无关文本

### 📊 完善的日志
- **实时进度显示**：收集、下载、写入三阶段进度实时反馈
- **详细错误信息**：完整的异常捕获和错误日志
- **性能统计**：显示总耗时和平均爬取速度

### 🛡️ 稳定可靠
- **自动重试机制**：请求失败自动重试，可配置重试次数
- **线程安全**：使用锁机制保护共享数据
- **优雅中断**：支持 Ctrl+C 安全退出

## 📋 系统要求

- Python 3.7+
- 依赖库：
 - `requests` >= 2.25.0
 - `beautifulsoup4` >= 4.9.0

## 🔧 安装

### 1. 克隆或下载脚本

```bash
# 下载脚本文件
scrape_novel.py
# 或直接复制代码到本地文件
```

### 2. 安装依赖

```bash
pip install requests beautifulsoup4
```

或使用 requirements.txt：

```bash
pip install -r requirements.txt
```

**requirements.txt 内容：**
```
requests>=2.25.0
beautifulsoup4>=4.9.0
```

## 🚀 快速开始

### 基本使用

```python
python scrape_novel.py
```

脚本将自动：
1. 从起始URL开始并发收集所有章节和页面
2. 并发下载所有页面内容
3. 按正确顺序整理并写入文件

### 自定义配置

编辑 `main()` 函数中的参数：

```python
def main():
    # 修改起始URL
    start_url = "https://m.shuhaige.net/你的小说ID/起始章节ID.html"
    
    # 修改输出文件名
    output_file = "我的小说.txt"
    
    # 创建爬虫实例
    spider = NovelSpider(start_url, output_file)
    spider.run()
```

## ⚙️ 配置说明

### Config 类参数

在 `Config` 类中可以调整以下参数：

```python
class Config:
    # 网络请求配置
    REQUEST_TIMEOUT = 15      # 请求超时时间（秒）
    MAX_RETRIES = 3           # 最大重试次数
    RETRY_DELAY = 2           # 重试延迟（秒）
    
    # 并发配置
    MAX_WORKERS = 8           # 下载内容时的并发线程数
    COLLECT_WORKERS = 4       # 收集URL时的并发线程数
    
    # 延迟配置
    COLLECT_DELAY = 0.05      # 收集URL时的延迟（秒）
    DOWNLOAD_DELAY = 0.05     # 下载内容时的延迟（秒）
    
    # 内容清理关键词
    FILTER_KEYWORDS = [
        'm.shuhaige.net', 
        '书海阁小说网', 
        '收藏', 
        '更新速度全网最快'
    ]
```

### 性能调优建议

| 场景 | COLLECT_WORKERS | MAX_WORKERS | 说明 |
|------|----------------|-------------|------|
| 保守模式 | 2 | 4 | 适合网络不稳定或担心被封IP |
| 平衡模式 | 4 | 8 | **推荐**，速度与稳定性兼顾 |
| 激进模式 | 8 | 16 | 网络良好且目标网站承受力强 |

## ⚠️ 注意事项

### 法律与道德

- ⚖️ **遵守法律**：请确保你的使用符合当地法律法规
- 📜 **尊重版权**：仅用于个人学习研究，不得用于商业用途
- 🤝 **遵守协议**：遵守目标网站的 robots.txt 和服务条款
- 🚫 **适度使用**：避免对网站服务器造成过大压力

### 技术限制

- 🌐 **网站特定**：脚本针对书海阁网站结构设计，其他网站可能需要修改
- 🔒 **反爬机制**：部分网站可能有验证码、IP限制等反爬措施
- 📡 **网络依赖**：需要稳定的网络连接
- 🔄 **结构变化**：网站HTML结构变化可能导致脚本失效

### 使用建议

1. **首次使用**：建议先用较小的并发数测试（COLLECT_WORKERS=2, MAX_WORKERS=4）
2. **监控日志**：注意观察日志输出，及时发现问题
3. **定期备份**：爬取过程中建议不要关闭终端
4. **错误处理**：如遇大量失败，降低并发数或增加延迟

## 🐛 常见问题


### Q: 如何加快爬取速度？
**A:**
1. 增加 `COLLECT_WORKERS` 和 `MAX_WORKERS`
2. 减少 `COLLECT_DELAY` 和 `DOWNLOAD_DELAY`
3. 但要注意可能被封IP的风险

### Q: 出现大量请求失败怎么办？
**A:**
1. 降低并发数
2. 增加延迟时间
3. 检查网络连接
4. 可能需要使用代理IP

### Q: 章节顺序错乱？
**A:** 最新版本已通过URL解析和排序解决此问题，确保使用最新代码。

### Q: 如何爬取其他网站的小说？
**A:** 需要修改 `parse_page()` 方法中的HTML解析逻辑，适配目标网站的结构。

## 📄 许可证

本项目提供 Apache-2.0 许可，仅供学习交流使用，请勿用于商业用途。



---

**免责声明**：本工具仅用于技术学习和研究目的。使用者需自行承担使用本工具的一切后果，开发者不对任何因使用本工具而产生的问题负责。
